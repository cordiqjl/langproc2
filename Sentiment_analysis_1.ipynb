{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import gensim\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_afinn(filename):\n",
    "    with open(filename) as w:\n",
    "        lines = [line.split('\\t') for line in w]\n",
    "    afinn = {}\n",
    "    for i, m in enumerate(lines):\n",
    "        afinn[m[0]] = int(m[1])\n",
    "    return afinn\n",
    "\n",
    "def line_sentiment(line, afinn):\n",
    "    words = line.lower().split()\n",
    "    i = 0\n",
    "    for each in words:\n",
    "        i += afinn.get(each, 0)\n",
    "    return i\n",
    "\n",
    "def get_data(path, path2):\n",
    "    filenames = []\n",
    "    data = {}\n",
    "    for root, dirs, files in walk(path):\n",
    "        for name in dirs:\n",
    "            pth = '//'.join((root, name))\n",
    "            filenames.append([(pth+'//'+f, f) for f in listdir(pth) if isfile(pth+'//'+f)])\n",
    "    for i in range(len(filenames[0])):\n",
    "        label = filenames[0][i][1].rsplit('.',1)[0]\n",
    "        for j in range(len(filenames)):\n",
    "            with open(filenames[j][i][0]) as f:\n",
    "                if j:\n",
    "                    data[label] += [line.split('\\n')[0] for line in f]\n",
    "                else:\n",
    "                    data[label] = [line.split('\\n')[0] for line in f]\n",
    "    pom = {}\n",
    "    filenames = []\n",
    "    for root, dirs, files in walk(path2):\n",
    "        for name in dirs:\n",
    "            if name == 'txt.parag':\n",
    "                pth = '//'.join((root, name))\n",
    "                filenames += [(pth+'//'+f, f) for f in listdir(pth) if isfile(pth+'//'+f)]\n",
    "\n",
    "    for i in range(len(filenames)):\n",
    "        with open(filenames[i][0]) as f:\n",
    "            pom[filenames[i][1].split('.')[0]] = ' '.join([line.split('\\n')[0] for line in f])\n",
    "    for i, m in enumerate(data.get('id')):\n",
    "        if i > 0:\n",
    "            data['full'].append(pom.get(m))\n",
    "        else:\n",
    "            pom.get(m)\n",
    "            data['full'] = [pom.get(m)]\n",
    "    return data\n",
    "\n",
    "def split(data, count, ratio=0.8):\n",
    "    test = {}\n",
    "    train = {}\n",
    "    keys = data.keys()\n",
    "    \n",
    "    shuffle = list(range(0,count))\n",
    "    random.shuffle(shuffle)\n",
    "    train_size = int(count * ratio)\n",
    "    test_size = count - train_size\n",
    "    \n",
    "    for i in shuffle[:train_size]:\n",
    "        for k in keys:\n",
    "            if k in train:\n",
    "                train[k].append(data[k][i])\n",
    "            else:\n",
    "                train[k] = [data[k][i]]\n",
    "\n",
    "    for i in shuffle[-test_size:]:\n",
    "        for k in keys:\n",
    "            if k in test:\n",
    "                test[k].append(data[k][i])\n",
    "            else:\n",
    "                test[k] = [data[k][i]]\n",
    "    return test, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afinn = get_afinn(\"AFINN-111.txt\")\n",
    "afinn_keys = [k for k in afinn]\n",
    "afinn_weights = np.array([afinn.get(key) for key in afinn_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_data('scaledata//scaledata//', 'scaledata//scale_whole_review//')\n",
    "#  id, label.3class, label.4class, rating, subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test, train = split(data, len(data['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(data, label):\n",
    "    x = []\n",
    "    y = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    all_words = []\n",
    "    stopw = set(stopwords.words('english'))\n",
    "    \n",
    "    x = [' '.join([wnl.lemmatize(w) for w in word_tokenize(d.lower()) if w not in stopw]) for d in data]\n",
    "    y = [int(l) for l in label]\n",
    "    for d in x:\n",
    "        for w in d.split():\n",
    "            all_words.append(w)\n",
    "    return all_words, x, y\n",
    "\n",
    "def featurecount(data):\n",
    "    d = set(data.split())\n",
    "    pom = {}\n",
    "    for f in features:\n",
    "        if f in d:\n",
    "            pom[f] = True\n",
    "        else:\n",
    "            pom[f] = False\n",
    "    return pom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_words_train, x_train, y_train = preprocess(train['full'], train['label.3class'])\n",
    "all_words_test, x_test, y_test = preprocess(test['full'], test['label.3class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CV = CountVectorizer(vocabulary=afinn_keys, analyzer='word')\n",
    "x_train_vec_cv = CV.fit_transform(x_train)\n",
    "x_test_vec_cv = CV.transform(x_test)\n",
    "\n",
    "for i, m in enumerate(afinn_weights):\n",
    "    x_train_vec_cv[:,i] = x_train_vec_cv[:,i]*m\n",
    "    x_test_vec_cv[:,i] = x_test_vec_cv[:,i]*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFINN - True, TFIDF - False\n",
      "Random forest classifier: 0.501996007984\n",
      "Support Vector classifier: 0.581836327345\n",
      "Linear Support Vector classifier: 0.532934131737\n",
      "Logistic Regression classifier: 0.559880239521\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print('AFINN - True, TFIDF - False')\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(x_train_vec_cv.toarray(), y_train)\n",
    "print('Random forest classifier:', forest.score(x_test_vec_cv.toarray(), y_test))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(x_train_vec_cv.toarray(), y_train)\n",
    "print('Support Vector classifier:', svc.score(x_test_vec_cv.toarray(), y_test))\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(x_train_vec_cv.toarray(), y_train)\n",
    "print('Linear Support Vector classifier:', lsvc.score(x_test_vec_cv.toarray(), y_test))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_vec_cv.toarray(), y_train)\n",
    "print('Logistic Regression classifier:', lr.score(x_test_vec_cv.toarray(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "TV = TfidfVectorizer(analyzer='word', min_df=0.05, max_df=0.9)\n",
    "x_train_vec_tv = TV.fit_transform(x_train)\n",
    "x_test_vec_tv = TV.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFINN - False, TFIDF - True\n",
      "Random forest classifier: 0.482035928144\n",
      "C-Support Vector classifier: 0.37624750499\n",
      "Linear Support Vector classifier: 0.631736526946\n",
      "Logistic Regression classifier: 0.62375249501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print('AFINN - False, TFIDF - True')\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(x_train_vec_tv.toarray(), y_train)\n",
    "print('Random forest classifier:', forest.score(x_test_vec_tv.toarray(), y_test))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(x_train_vec_tv.toarray(), y_train)\n",
    "print('C-Support Vector classifier:', svc.score(x_test_vec_tv.toarray(), y_test))\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(x_train_vec_tv.toarray(), y_train)\n",
    "print('Linear Support Vector classifier:', lsvc.score(x_test_vec_tv.toarray(), y_test))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_vec_tv.toarray(), y_train)\n",
    "print('Logistic Regression classifier:', lr.score(x_test_vec_tv.toarray(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CV = CountVectorizer(analyzer='word', min_df=0.05, max_df=0.9)\n",
    "x_train_vec_cv2 = CV.fit_transform(x_train)\n",
    "x_test_vec_cv2 = CV.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFINN - False, TFIDF - False\n",
      "Random forest classifier: 0.494011976048\n",
      "Support Vector classifier: 0.634730538922\n",
      "Linear Support Vector classifier: 0.588822355289\n",
      "Logistic Regression classifier: 0.597804391218\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print('AFINN - False, TFIDF - False')\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(x_train_vec_cv2.toarray(), y_train)\n",
    "print('Random forest classifier:', forest.score(x_test_vec_cv2.toarray(), y_test))\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "svc.fit(x_train_vec_cv2.toarray(), y_train)\n",
    "print('Support Vector classifier:', svc.score(x_test_vec_cv2.toarray(), y_test))\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lsvc = LinearSVC()\n",
    "lsvc.fit(x_train_vec_cv2.toarray(), y_train)\n",
    "print('Linear Support Vector classifier:', lsvc.score(x_test_vec_cv2.toarray(), y_test))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_vec_cv2.toarray(), y_train)\n",
    "print('Logistic Regression classifier:', lr.score(x_test_vec_cv2.toarray(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequent_words = nltk.FreqDist(all_words_train)\n",
    "features = list(sorted(frequent_words, key=frequent_words.get, reverse=True))[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaiveBayesClassifier score: 0.6127744510978044\n",
      "MultinomialNB score: 0.607784431138\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "train_set = [(featurecount(m), y_train[i]) for i, m in enumerate(x_train)]\n",
    "test_set = [(featurecount(m), y_test[i]) for i, m in enumerate(x_test)]\n",
    "\n",
    "NBC = NaiveBayesClassifier.train(train_set)\n",
    "print('NaiveBayesClassifier score:', nltk.classify.accuracy(NBC, test_set))\n",
    "\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "MNB = SklearnClassifier(MultinomialNB())\n",
    "MNB.train(train_set)\n",
    "print('MultinomialNB score:', nltk.classify.accuracy(MNB, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def voting(x_train, y_train, x_test, y_test, all_words_train, afinn_keys, afinn_weights):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import LinearSVC\n",
    "    from sklearn.svm import SVC\n",
    "    from nltk import NaiveBayesClassifier\n",
    "    from nltk.classify.scikitlearn import SklearnClassifier\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "    # Sklearn classifiers\n",
    "    lr_tv = LogisticRegression()\n",
    "    lr_cv = LogisticRegression()\n",
    "    svc = SVC()\n",
    "    lsvc_cv = LinearSVC()\n",
    "    lsvc_tv = LinearSVC()\n",
    "    \n",
    "    # Vectorization\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    TV = TfidfVectorizer(analyzer='word', min_df=0.05, max_df=0.8)\n",
    "    x_train_vec_tv = TV.fit_transform(x_train)\n",
    "    x_test_vec_tv = TV.transform(x_test)\n",
    "    \n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    CV = CountVectorizer(vocabulary=afinn_keys, analyzer='word')\n",
    "    x_train_vec_cv = CV.fit_transform(x_train)\n",
    "    x_test_vec_cv = CV.transform(x_test)\n",
    "\n",
    "    for i, m in enumerate(afinn_weights):\n",
    "        x_train_vec_cv[:,i] = x_train_vec_cv[:,i]*m\n",
    "        x_test_vec_cv[:,i] = x_test_vec_cv[:,i]*m\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    lsvc_cv.fit(x_train_vec_cv.toarray(), y_train)    \n",
    "    lsvc_tv.fit(x_train_vec_tv.toarray(), y_train)    \n",
    "    lr_tv.fit(x_train_vec_tv.toarray(), y_train)\n",
    "    lr_cv.fit(x_train_vec_cv.toarray(), y_train)\n",
    "    svc.fit(x_train_vec_cv.toarray(), y_train)\n",
    "    \n",
    "    # Naive Bayes training\n",
    "    frequent_words = nltk.FreqDist(all_words_train)\n",
    "    features = list(sorted(frequent_words, key=frequent_words.get, reverse=True))[:5000]\n",
    "    \n",
    "    train_set = [(featurecount(m), y_train[i]) for i, m in enumerate(x_train)]\n",
    "    test_set = [(featurecount(m), y_test[i]) for i, m in enumerate(x_test)]\n",
    "    \n",
    "    NBC = NaiveBayesClassifier.train(train_set)\n",
    "    MNB = SklearnClassifier(MultinomialNB())\n",
    "    MNB.train(train_set)\n",
    "    \n",
    "    # Vote generating\n",
    "    lst = []\n",
    "    lst.append(lsvc_cv.predict(x_test_vec_cv.toarray()))   \n",
    "    lst.append(lsvc_tv.predict(x_test_vec_tv.toarray()))    \n",
    "    lst.append(lr_tv.predict(x_test_vec_tv.toarray()))\n",
    "    lst.append(lr_cv.predict(x_test_vec_cv.toarray()))\n",
    "    lst.append(svc.predict(x_test_vec_cv.toarray()))\n",
    "    lst.append([MNB.classify(test_set[i][0]) for i in range(len(test_set))])\n",
    "    lst.append([NBC.classify(test_set[i][0]) for i in range(len(test_set))])\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = voting(x_train, y_train, x_test, y_test, all_words_train, afinn_keys, afinn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def vote_count(final):\n",
    "    fin = []\n",
    "    for i in range(len(final[0])):\n",
    "        pom = []\n",
    "        for each in final:\n",
    "            pom.append(each[i])\n",
    "        #fin.append(most_common(pom))\n",
    "        fin.append(int(round(sum(pom)/len(final)-0.00001)))\n",
    "    return fin\n",
    "\n",
    "def voting_score(y_test, final):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    pom = vote_count(final)\n",
    "    return accuracy_score(y_test, pom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def voting_score(y_test, final):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    pom = vote_count(final)\n",
    "    \n",
    "    return accuracy_score(y_test, pom), f1_score(y_test, pom, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accu, f1 = voting_score(y_test, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65569\n"
     ]
    }
   ],
   "source": [
    "print(\"%.5f\" % accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65508\n"
     ]
    }
   ],
   "source": [
    "print(\"%.5f\" % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pom = vote_count(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 505 339\n",
      "247 377 378\n",
      "\n",
      "Slight bias against negative sentiment can be seen. This could be expected, due to an imbalance in the data set as a whole.\n",
      "\n",
      "Also, due to the nature of the voting system implemented, neutral values have a large bias.\n",
      "We could probably get more meaningful results by implementing a confidence measure,\n",
      "which would allow us to asses towards which sentiment the neutral documents lean.\n"
     ]
    }
   ],
   "source": [
    "print(len([1 for i in pom if i == 0]),\n",
    "      len([1 for i in pom if i == 1]),\n",
    "      len([1 for i in pom if i == 2]))\n",
    "print(len([1 for i in y_test if i == 0]),\n",
    "      len([1 for i in y_test if i == 1]),\n",
    "      len([1 for i in y_test if i == 2]))\n",
    "print()\n",
    "print('Slight bias against negative sentiment can be seen. This could be expected, due to an imbalance in the data set as a whole.')\n",
    "print()\n",
    "print('Also, due to the nature of the voting system implemented, neutral values have a large bias.')\n",
    "print('We could probably get more meaningful results by implementing a confidence measure,')\n",
    "print('which would allow us to asses towards which sentiment the neutral documents lean.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
