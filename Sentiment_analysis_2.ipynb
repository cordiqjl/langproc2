{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\lib\\site-packages\\gensim-2.1.0-py3.5-win-amd64.egg\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy\n",
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import gensim\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_afinn(filename):\n",
    "    with open(filename) as w:\n",
    "        lines = [line.split('\\t') for line in w]\n",
    "    afinn = {}\n",
    "    for i, m in enumerate(lines):\n",
    "        afinn[m[0]] = int(m[1])\n",
    "    return afinn\n",
    "\n",
    "def line_sentiment(line, afinn):\n",
    "    words = line.lower().split()\n",
    "    i = 0\n",
    "    for each in words:\n",
    "        i += afinn.get(each, 0)\n",
    "    return i\n",
    "\n",
    "def get_data(path, path2):\n",
    "    filenames = []\n",
    "    data = {}\n",
    "    for root, dirs, files in walk(path):\n",
    "        for name in dirs:\n",
    "            pth = '//'.join((root, name))\n",
    "            filenames.append([(pth+'//'+f, f) for f in listdir(pth) if isfile(pth+'//'+f)])\n",
    "    for i in range(len(filenames[0])):\n",
    "        label = filenames[0][i][1].rsplit('.',1)[0]\n",
    "        for j in range(len(filenames)):\n",
    "            with open(filenames[j][i][0]) as f:\n",
    "                if j:\n",
    "                    data[label] += [line.split('\\n')[0] for line in f]\n",
    "                else:\n",
    "                    data[label] = [line.split('\\n')[0] for line in f]\n",
    "    pom = {}\n",
    "    filenames = []\n",
    "    for root, dirs, files in walk(path2):\n",
    "        for name in dirs:\n",
    "            if name == 'txt.parag':\n",
    "                pth = '//'.join((root, name))\n",
    "                filenames += [(pth+'//'+f, f) for f in listdir(pth) if isfile(pth+'//'+f)]\n",
    "\n",
    "    for i in range(len(filenames)):\n",
    "        with open(filenames[i][0]) as f:\n",
    "            pom[filenames[i][1].split('.')[0]] = ' '.join([line.split('\\n')[0] for line in f])\n",
    "    for i, m in enumerate(data.get('id')):\n",
    "        if i > 0:\n",
    "            data['full'].append(pom.get(m))\n",
    "        else:\n",
    "            pom.get(m)\n",
    "            data['full'] = [pom.get(m)]\n",
    "    return data\n",
    "\n",
    "def split(data, count, ratio=0.8):\n",
    "    test = {}\n",
    "    train = {}\n",
    "    keys = data.keys()\n",
    "    \n",
    "    shuffle = list(range(0,count))\n",
    "    random.shuffle(shuffle)\n",
    "    train_size = int(count * ratio)\n",
    "    test_size = count - train_size\n",
    "    \n",
    "    for i in shuffle[:train_size]:\n",
    "        for k in keys:\n",
    "            if k in train:\n",
    "                train[k].append(data[k][i])\n",
    "            else:\n",
    "                train[k] = [data[k][i]]\n",
    "\n",
    "    for i in shuffle[-test_size:]:\n",
    "        for k in keys:\n",
    "            if k in test:\n",
    "                test[k].append(data[k][i])\n",
    "            else:\n",
    "                test[k] = [data[k][i]]\n",
    "    return test, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "afinn = get_afinn(\"AFINN-111.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = get_data('scaledata//scaledata//', 'scaledata//scale_whole_review//')\n",
    "#  id, label.3class, label.4class, rating, subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test, train = split(data, len(data['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess(data, label):\n",
    "    x = []\n",
    "    y = []\n",
    "    wnl = WordNetLemmatizer()\n",
    "    all_words = set()\n",
    "    stopw = set(stopwords.words('english'))\n",
    "    \n",
    "    x = [[wnl.lemmatize(w) for w in word_tokenize(d.lower()) if w not in stopw] for d in data]\n",
    "    y = [l for l in label]\n",
    "    for d in x:\n",
    "        for w in d:\n",
    "            all_words.add(w)\n",
    "    return all_words, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_wordsx, x_train, y_train = preprocess(train['full'], train['label.3class'])\n",
    "all_wordsy, x_test, y_test = preprocess(test['full'], test['label.3class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
